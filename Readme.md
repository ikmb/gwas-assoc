# IKMB GWAS Association Testing Pipeline

## Prerequisites
- Nextflow: https://www.nextflow.io/
- Java 8 or higher
- Singularity 3.4 or higher
- A dataset in chromosome-wise .vcf.gz format. The dataset *must have* a DS tag for dosage data or a GT tag for genotyped data. If both are present, DS is chosen. 
- Annotations and FAM files as generated by the QC Pipeline (i.e. `dataset_individuals_annotation.txt` and `dataset.fam`)
- Optionally, a whitespace-separated file with additional covariates for association testing (see below)

Please ensure that you have 16 GB RAM installed on the computer where you intend to run the pipeline (i.e. your local computer or your HPC compute nodes).

## Quick Start

â†’ If you are working on the UKSH medcluster, please see "Advanced Configuration" first.

Fortunately, the association testing pipeline itself requires very little configuration. If you intend to run the pipeline on an HPC cluster, please review the advanced configuration items first.

You will need:
- A set of `.vcf.gz` files with the following specifics:
    - at most one chromosome per `.vcf.gz` file. Multiple chromosomes per files are not supported. If a file happens to have multiple chromosomes, only the first will be analyzed.
    - any chromosome codes are supported (i.e. `chrX`, `X`, `23`, `chr23`, `chromosomeX` are just fine)
    - your VCF files require dosage data (DS tag) for imputed data or genotype calls (GT tag) for genotyped data. If both tags are found, DS is chosen.
    - the `INFO` column in the VCF files should contain an imputation score. This is used to filter the input variants to create a good null model for SAIGE. For topmed imputations, we found `R2>0.8` to yield good results. This filter is not used for Plink-based association testing. 
- Individuals annotations as created by the QC counterpart. Please see [the manual](https://github.com/ikmb/gwas-qc/Readme.md) on how to create one if you didn't use the QC pipeline before imputation. Otherwise, you can use the annotations found in the `QCed` output folder.
- Optionally, you can specify a FAM file as a sample filter. Only those files in the FAM file will be used from the VCFs. Note that *all* samples from the FAM file must be present in the VCF files.
- Optionally, you can specify additional covariates to be used in association testing. By default, 10 principal components are automatically generated and used. If you want additional covariates, have a whitespace-separated file with a header at hand. The first column should be the sample ID in `FID_IID` format, any futher column is treated as a covariate. Specify the covars file with `--more_covars $FILE` and the columns to be used with `--more_covars_cols AGE,SEX`, where `AGE` and `SEX` are the respective column headers from the covar file that you wish to be included.

```bash
# For additional clarity, we use variables here.

# a shell-like glob expression for specifying VCF.GZ file sets.
# Note the additional quoting, we do not want the shell to expand the '*'
INPUT="$HOME/Example/QCed/"'*.noATCG.vcf.gz'

# Filter by these FAM files. In our example, they are the same as in the VCF
FAM="$HOME/Example/QCed/1000G_QCed.fam"

# Individuals annotation as generated by the QC Pipeline
ANNO="$HOME/Example/QCed/1000G_QCed.annotations.txt"

# Name prefix of the output files
NAME="1000G"

# Target folder
OUTPUT="Assoc_output"

# Actual call
# The Assoc.config defines computation resources and can be fine-tuned if necessary. You can
# use the generic one from 
# https://raw.githubusercontent.com/ikmb/gwas-assoc/master/Assoc.config

nextflow run -c Assoc.config ikmb/gwas-assoc \
    --input_imputed_glob $INPUT \
    --fam $FAM \
    --anno $ANNO \
    --collection_name $NAME \
    --output Assoc_output
```

The pipeline output and reports will be written to the ```Assoc_output``` directory.


## Advanced Configuration

### Local and Side-wide Configuration

You can extend the Nextflow default configuration by using (or creating) the following configuration files:
- a ```nextflow.config``` in the current project folder. This file is automatically read if Nextflow is launched from this folder.
- a site-wide ```$HOME/.nextflow/config```. This file is automatically read every time you run Nextflow

It is usually a good idea to place additional configuration items to the side-wide configuration (see below for examples).

### Shared Singularity Cache

In a shared compute enviroment such as HPC clusters, it is often useful to share the singularity cache with other Nextflow/Singularity users so they would not have to download the same large files over and over again. To specify the singularity cache directory, add the following line to your config:
```
singularity.cacheDir = '/some/shared/place/singularity'
```
Note that the directory must be accessible from all compute nodes.

### HPC Resources and Job Queues

By default, all processes are launched on the computer where the QC is started. This is usually not appropriate on HPC login nodes where jobs should be sheduled on different nodes. Nextflow provides support for a broad range of job submission systems, such as SLURM, SGE or PBS/Torque. Please review the [Nextflow documenation on compute resources](https://www.nextflow.io/docs/latest/executor.html).

For example, if you intend to use a SLURM environment, place the following code in your config (see "Shared Singularity Cache"):
```
process.executor = "slurm"
executor.queue = "partition-name"  // optional
executor.queueSize = 150           // optional, override max queue length for Nextflow
process.clusterOptions = '--qos=someval' // optional, if you need to supply additional args to sbatch
```

### Mounting Paths into the Singularity Container

To separate the operating system within the singularity container from the host system, Singularity only makes your home folder accessible to the insides. If your data files are stored in a directory different from your home (e.g. a shared storage ```/data_storage```), you will need to explicitly make it accessible by specifying additional singularity options in your config file:
```
singularity.runOptions = "-B /data_storage -B /some_other_storage -B /even_more_storage"
```

### UKSH medcluster configuration

For optimal configuration on the UKSH medcluster, perform the following configuration changes (you only need to do this once):

Create or change your `$HOME/.nextflow/config` file:
```
// use pre-populated singularity image cache
singularity.cacheDir = "/work_ifs/sukmb388/singularity-cache"

// bind /work_ifs folders. If you need more than $HOME and work_ifs, add another "-B /somewhere" switch.
singularity.runOptions = "-B /work_ifs"

// make nextflow use slurm by default
profiles {
    standard {
        executor.name = "slurm"
        executor.queueSize = 150
        process.executor = "slurm"
        process.queue = "all"
    }
}
```

