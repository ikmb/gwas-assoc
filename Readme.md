# IKMB GWAS Association Testing Pipeline

## Prerequisites
- Nextflow: https://www.nextflow.io/
- Java 8 or higher
- Singularity 3.4 or higher
- A dataset in chromosome-wise .vcf.gz format. The dataset *must have* a DS tag for dosage data or a GT tag for genotyped data. If both are present, DS is chosen. 
- Annotations and FAM files as generated by the QC Pipeline (i.e. `dataset_individuals_annotation.txt` and `dataset.fam`)
- Optionally, a whitespace-separated file with additional covariates for association testing (see below)

Please ensure that you have 16 GB RAM installed on the computer where you intend to run the pipeline (i.e. your local computer or your HPC compute nodes).

## Quick Start

If you are working on the UKSH medcluster, please see "Advanced Configuration" first.

1. Get the example dataset: https://raw.githubusercontent.com/ikmb/gwas-qc/master/Example.tar.gz
2. Unpack it in your home folder: ```tar xvaf Example.tar.gz -C $HOME; cd $HOME/Example```
3. Launch the pipeline:
```bash
nextflow run -c Assoc.config ikmb/gwas-assoc \
    --input_imputed_glob $HOME/Example/QCed/'*.noATCG.vcf.gz' \
    --fam $HOME/Example/QCed/1000G_QCed.fam \
    --anno $HOME/Example/QCed/1000G_QCed.annotations.txt \
    --collection_name 1000G_Example \
    --output Assoc_output
```

The pipeline output and reports will be written to the ```Assoc_output``` directory.

## Advanced Configuration

### Local and Side-wide Configuration

You can extend the Nextflow default configuration by using (or creating) the following configuration files:
- a ```nextflow.config``` in the current project folder. This file is automatically read if Nextflow is launched from this folder.
- a site-wide ```$HOME/.nextflow/config```. This file is automatically read every time you run Nextflow

It is usually a good idea to place additional configuration items to the side-wide configuration (see below for examples).

### Shared Singularity Cache

In a shared compute enviroment such as HPC clusters, it is often useful to share the singularity cache with other Nextflow/Singularity users so they would not have to download the same large files over and over again. To specify the singularity cache directory, add the following line to your config:
```
singularity.cacheDir = '/some/shared/place/singularity'
```
Note that the directory must be accessible from all compute nodes.

### HPC Resources and Job Queues

By default, all processes are launched on the computer where the QC is started. This is usually not appropriate on HPC login nodes where jobs should be sheduled on different nodes. Nextflow provides support for a broad range of job submission systems, such as SLURM, SGE or PBS/Torque. Please review the [Nextflow documenation on compute resources](https://www.nextflow.io/docs/latest/executor.html).

For example, if you intend to use a SLURM environment, place the following code in your config (see "Shared Singularity Cache"):
```
process.executor = "slurm"
executor.queue = "partition-name"  // optional
executor.queueSize = 150           // optional, override max queue length for Nextflow
process.clusterOptions = '--qos=someval' // optional, if you need to supply additional args to sbatch
```

### Mounting Paths into the Singularity Container

To separate the operating system within the singularity container from the host system, Singularity only makes your home folder accessible to the insides. If your data files are stored in a directory different from your home (e.g. a shared storage ```/data_storage```), you will need to explicitly make it accessible by specifying additional singularity options in your config file:
```
singularity.runOptions = "-B /data_storage -B /some_other_storage -B /even_more_storage"
```

### UKSH medcluster configuration

For optimal configuration on the UKSH medcluster, perform the following configuration changes (you only need to do this once):

Create or change your `$HOME/.nextflow/config` file:
```
// use pre-populated singularity image cache
singularity.cacheDir = "/work_ifs/sukmb388/singularity-cache"

// bind /work_ifs folders. If you need more than $HOME and work_ifs, add another "-B /somewhere" switch.
singularity.runOptions = "-B /work_ifs"

// make nextflow use slurm by default
profiles {
    standard {
        executor.name = "slurm"
        executor.queueSize = 150
        process.executor = "slurm"
        process.queue = "all"
    }
}
```

